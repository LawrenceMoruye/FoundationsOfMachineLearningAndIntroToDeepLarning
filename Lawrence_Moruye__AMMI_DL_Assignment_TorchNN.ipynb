{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lawrence_Moruye_ AMMI_DL_Assignment_TorchNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAkBSdTvq0Cr"
      },
      "source": [
        "<img src='http://sn.nexteinstein.org/wp-content/uploads/sites/12/2016/07/aims_senegal.jpg' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSBZZYqgq2Su"
      },
      "source": [
        "In this notebook you will implement and train a feed-forward neural network using the torch.nn module to solve a multi-class classification problem on a dataset called \"Fashion MNIST\".\n",
        "\n",
        "---\n",
        "\n",
        "***Learning Objectives:***\n",
        "\n",
        "* Understand how to use Pytorch nn module and the sequential container to build a neural network architecture.\n",
        "\n",
        "\n",
        "* Understand how a model is trained using the opitm module and the Autgrad engine and How to evaluate your model.\n",
        "\n",
        "---\n",
        "\n",
        "**Resources that you need to cover before starting**:\n",
        "\n",
        "* Review the Tutorial Notebook.\n",
        "\n",
        "* Read the torch.nn docs: https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "* Read the article about moving from computing gradient from scratch to useing torch.nn https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
        "\n",
        "* Read the pytorch offical tutorial on training multi-class classifiers:\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4ngU0vOrd68"
      },
      "source": [
        "# Setup code (Read first But Don't modify)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NljcxD0Estne"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MygxOwpX-H4f"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQKtRDqBz3gT"
      },
      "source": [
        "#### Loading the Data: \n",
        "\n",
        "We will use the Fashion MNIST dataset consisting of 70,000 greyscale images of shape(28x28 flattened to have shape 784) and their labels.\n",
        "\n",
        "The dataset is divided into 60,000 training images and 10,000 test images. The idea is to train a classifier to identify the class value (what type of fashion item it is) given the image (total of **10** classes).\n",
        "\n",
        "We will split the training data into 50,000 training images used to train a model on it, and 10,000 validation images used to tune your choice of hyperparams and model architecture.\n",
        "\n",
        "The last step is to evaluate how well your model classifies the 10,000 test images (will be used to assess your implementation).\n",
        "\n",
        "\n",
        "**The classes are: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']**\n",
        "\n",
        "This is how the 10 class images looks like before flattening it:\n",
        "\n",
        "<img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F549262%2Fd6f4f6e13fa211c9e773479566d89ac9%2FExample-for-fashion-MNIST-Each-class-is-represented-by-nine-cases.png?generation=1576784453715625&alt=media' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyVLHX__-Jws"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX1jXWxtweco"
      },
      "source": [
        "#get the data\n",
        "\n",
        "def load_data():\n",
        "\n",
        "  #using the dataset module from torchvision\n",
        "  mnist_train_set = datasets.FashionMNIST('data/fashiomnist/', train = True, download = True)\n",
        "  mnist_test_set = datasets.FashionMNIST('data/fashiomnist/', train = False, download = True)\n",
        "\n",
        "  #train data\n",
        "  train_images = mnist_train_set.data.view(-1, 1, 28, 28).float()\n",
        "  train_targets = mnist_train_set.targets\n",
        "\n",
        "  #test data\n",
        "  test_images = mnist_test_set.data.view(-1, 1, 28, 28).float()\n",
        "  y_test = mnist_test_set.targets\n",
        "\n",
        "  #flatten\n",
        "  train_val_input = train_images.clone().reshape(train_images.size(0), -1)/255.0\n",
        "  x_test = test_images.clone().reshape(test_images.size(0), -1)/255.0\n",
        "\n",
        "  # shuffle\n",
        "  N = train_val_input.shape[0]\n",
        "  index = torch.randperm(N)\n",
        "\n",
        "  #train val split\n",
        "  x_train = train_val_input[index][:50000]\n",
        "  y_train = train_targets[index][:50000]\n",
        "\n",
        "  x_val = train_val_input[index][50000:]\n",
        "  y_val = train_targets[index][50000:]\n",
        "\n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmO0FsLnYxpJ"
      },
      "source": [
        "x_train, y_train, x_val, y_val, _, __ = load_data()"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHajRlnh8ZBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef47f9d2-9ce9-48f1-8e65-cca527ba70b8"
      },
      "source": [
        "print('train data shape: ', x_train.shape)\n",
        "print('train targets shape: ', y_train.shape)\n",
        "print(' ')\n",
        "print('validation data shape: ', x_val.shape)\n",
        "print('Validation targets shape: ', y_val.shape)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data shape:  torch.Size([50000, 784])\n",
            "train targets shape:  torch.Size([50000])\n",
            " \n",
            "validation data shape:  torch.Size([10000, 784])\n",
            "Validation targets shape:  torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7RcqSEetVpE"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qVF8ZdO-MkE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpfIjcho6PIV"
      },
      "source": [
        "#### We will be using gpus to accelerate the training. make sure to change your runtime to use a *gpu*.\n",
        "\n",
        "#### go to Runtime ===> Change Runtime Type ===> Hardware accelerator ===> choose (GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA5llWY9te4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c824f3-cea0-479e-d8a1-85ab5c4265d7"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  \n",
        "print('using device:', device)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZgAMtBSZHqA"
      },
      "source": [
        "Note that every tensor in pytorch has a **.device** attribute By default it's set to 'cpu' But it can be changed to 'gpu' using the **'.to'** command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La4h7iZnZbiR"
      },
      "source": [
        "# toy example \n",
        "a = torch.rand(2,3)\n",
        "print(a.device) # by default it's 'cpu'\n",
        "\n",
        "a = a.to(device = device)\n",
        "print(a.device) # should print 'cuda:0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN2tXNtr9-vX"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-ExNrjaAU5"
      },
      "source": [
        "Now it's your turn to define a model using the **torch.nn.Sequential** container module.\n",
        "\n",
        "Your model should accept an input of shape D = 784 and output 10 numbers represent the probability of belonging to each class.\n",
        "\n",
        "ideally a good model will have at least one hidden layer (or more) followed by non-linear activation functions like (nn.ReLU, nn.Sigmoid ... etc) \n",
        "\n",
        "\n",
        "you can take some help from the docs: \n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IqWt4DHtf_O"
      },
      "source": [
        "# define a neural network model using the nn.Sequential container\n",
        "\n",
        "D = x_train.shape[1]\n",
        "n_classes = 10\n",
        "H1=32\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D,H1),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H1,n_classes),\n",
        "    nn.Sigmoid()\n",
        "    #your code should be here  \n",
        "\n",
        ")"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7-cOz-vcKM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b513834-4d50-4254-c633-5a64365ce767"
      },
      "source": [
        "print(model) #Run this when you finish defining your model\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (3): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4NkC5mRb-go"
      },
      "source": [
        "#Run This cell\n",
        "# it's important to transfer your model to the gpu for faster computations\n",
        "\n",
        "model = model.to(device=device)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0hV6tQTcVNt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snZysKgKcekB"
      },
      "source": [
        "Now that we have our model and data ready, we need to train it: \n",
        "\n",
        "But in order to do that we need to define our loss function:\n",
        "\n",
        "luckly pytorch has most loss functions already implemented, for multi-class classification problems, we will be using the cross entroy loss which is basically computing the log softmax of an input followed by computing the negative log liklihood loss.\n",
        "\n",
        "read this for more info on the cross entropy loss: https://en.wikipedia.org/wiki/Cross_entropy\n",
        "\n",
        "also check the pytorch definition of it: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html (**important**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM_r6ITEwfov"
      },
      "source": [
        "#run this cell\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVQiq0zveZQj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7Wdomjejk9"
      },
      "source": [
        "Another important step in training any model is to update it's weights and biases, and for that we need an optimization algorithm like Gradient decsent, Luckily in pytorch most optimization algorithms are implemented in the torch.optim module, and for this assignment we will be using the optim.SGD But you are free to look into more optim in the doc: https://pytorch.org/docs/stable/optim.html.\n",
        "\n",
        "You need to find suitable values for the learning rate and the momentum value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73N2vqTWd_iC"
      },
      "source": [
        "learning_rate =0.1         # to be set\n",
        "momentum_value =0       # to be set\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum_value)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io1LkA5Ufzpp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Q9c1eaf3IY"
      },
      "source": [
        "# Now that we have all things set, let's train our model using Automatic differentaition from pytorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIUCO2XAwfr2"
      },
      "source": [
        "batch_size =32              #set your batch size "
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUa8WLW1gO8k"
      },
      "source": [
        "#Run this cell \n",
        "# it wil set the number of batches for mini batch SGD\n",
        "\n",
        "N = x_train.shape[0]\n",
        "\n",
        "if N % batch_size == 0:\n",
        "  n_batches = int(N / batch_size)\n",
        "else:\n",
        "  n_batches = int(N / batch_size) + 1"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARNONOwUguAl"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1MbG4fFgwBy"
      },
      "source": [
        "# Main Training loop\n",
        "### ToDo: fill in the missing steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Gd17hRwfvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db7b1c0-a249-4c1e-e885-5c812ebb802d"
      },
      "source": [
        "n_epochs =500     # set the number of epochs\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  \n",
        "  for batch in range(n_batches):\n",
        "    \n",
        "    start = batch * batch_size\n",
        "    end = start + batch_size\n",
        "\n",
        "    train_images = x_train[start:end]\n",
        "    train_labels = y_train[start:end]\n",
        "\n",
        "    train_images =train_images.to(device)        # change the device of train_images to gpu\n",
        "    train_labels = train_labels.to(device)       # change the device of train_labels to gpu\n",
        "\n",
        "    outputs =model(train_images)       # feed the images totrain_labels the model to get the outputs\n",
        "\n",
        "    loss =criterion(outputs,train_labels)          # compute the loss using the criterion we defined\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    # zero the grad\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # compute the gradient for each weight/bias from the loss\n",
        "    \n",
        "    # update the model parameters\n",
        "\n",
        "  epoch_loss = epoch_loss/N\n",
        "\n",
        "  print('epoch ==> ', epoch, 'loss ==> ', epoch_loss)\n",
        "\n",
        "  losses.append(epoch_loss)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch ==>  0 loss ==>  0.05489528424263\n",
            "epoch ==>  1 loss ==>  0.05206700759887695\n",
            "epoch ==>  2 loss ==>  0.05163631984233856\n",
            "epoch ==>  3 loss ==>  0.05134054612636566\n",
            "epoch ==>  4 loss ==>  0.05106036957740784\n",
            "epoch ==>  5 loss ==>  0.05085627458572388\n",
            "epoch ==>  6 loss ==>  0.05070881497144699\n",
            "epoch ==>  7 loss ==>  0.05059350829362869\n",
            "epoch ==>  8 loss ==>  0.05046270713567734\n",
            "epoch ==>  9 loss ==>  0.05010139660835266\n",
            "epoch ==>  10 loss ==>  0.05001070276975632\n",
            "epoch ==>  11 loss ==>  0.04994710005760193\n",
            "epoch ==>  12 loss ==>  0.04989417906999588\n",
            "epoch ==>  13 loss ==>  0.04984745400428772\n",
            "epoch ==>  14 loss ==>  0.04980557265281677\n",
            "epoch ==>  15 loss ==>  0.04976561424016952\n",
            "epoch ==>  16 loss ==>  0.04972550884485245\n",
            "epoch ==>  17 loss ==>  0.049677947046756744\n",
            "epoch ==>  18 loss ==>  0.049623629856109616\n",
            "epoch ==>  19 loss ==>  0.04956872435092926\n",
            "epoch ==>  20 loss ==>  0.04951995601177216\n",
            "epoch ==>  21 loss ==>  0.04947796198129654\n",
            "epoch ==>  22 loss ==>  0.04944265705108643\n",
            "epoch ==>  23 loss ==>  0.04941087443590164\n",
            "epoch ==>  24 loss ==>  0.04938253328561783\n",
            "epoch ==>  25 loss ==>  0.0493558992099762\n",
            "epoch ==>  26 loss ==>  0.049330858488082885\n",
            "epoch ==>  27 loss ==>  0.04930726335763931\n",
            "epoch ==>  28 loss ==>  0.04928637452840805\n",
            "epoch ==>  29 loss ==>  0.04926620635509491\n",
            "epoch ==>  30 loss ==>  0.0492475430059433\n",
            "epoch ==>  31 loss ==>  0.04922947998285294\n",
            "epoch ==>  32 loss ==>  0.049212139112949374\n",
            "epoch ==>  33 loss ==>  0.0491962167596817\n",
            "epoch ==>  34 loss ==>  0.049180402610301974\n",
            "epoch ==>  35 loss ==>  0.049165281329154965\n",
            "epoch ==>  36 loss ==>  0.04915174042224884\n",
            "epoch ==>  37 loss ==>  0.04913817447423935\n",
            "epoch ==>  38 loss ==>  0.04912499557971954\n",
            "epoch ==>  39 loss ==>  0.0491126703453064\n",
            "epoch ==>  40 loss ==>  0.04910027436494827\n",
            "epoch ==>  41 loss ==>  0.04908909120082855\n",
            "epoch ==>  42 loss ==>  0.04907790081977844\n",
            "epoch ==>  43 loss ==>  0.049066507136821744\n",
            "epoch ==>  44 loss ==>  0.049055653886795045\n",
            "epoch ==>  45 loss ==>  0.04904536002159118\n",
            "epoch ==>  46 loss ==>  0.049035081658363344\n",
            "epoch ==>  47 loss ==>  0.04902503592252731\n",
            "epoch ==>  48 loss ==>  0.049015338459014894\n",
            "epoch ==>  49 loss ==>  0.049004946422576905\n",
            "epoch ==>  50 loss ==>  0.04899559105396271\n",
            "epoch ==>  51 loss ==>  0.04898611452102661\n",
            "epoch ==>  52 loss ==>  0.0489774395942688\n",
            "epoch ==>  53 loss ==>  0.04896824097633362\n",
            "epoch ==>  54 loss ==>  0.0489592235827446\n",
            "epoch ==>  55 loss ==>  0.04895158413410187\n",
            "epoch ==>  56 loss ==>  0.04894263907194137\n",
            "epoch ==>  57 loss ==>  0.048934084980487826\n",
            "epoch ==>  58 loss ==>  0.04892676838874817\n",
            "epoch ==>  59 loss ==>  0.048918130974769596\n",
            "epoch ==>  60 loss ==>  0.0489108304309845\n",
            "epoch ==>  61 loss ==>  0.048904020969867706\n",
            "epoch ==>  62 loss ==>  0.048896579675674436\n",
            "epoch ==>  63 loss ==>  0.0488891695523262\n",
            "epoch ==>  64 loss ==>  0.04888212609767914\n",
            "epoch ==>  65 loss ==>  0.04887539170980453\n",
            "epoch ==>  66 loss ==>  0.04886792400360107\n",
            "epoch ==>  67 loss ==>  0.04886073741912842\n",
            "epoch ==>  68 loss ==>  0.04885343875408173\n",
            "epoch ==>  69 loss ==>  0.04884644170284271\n",
            "epoch ==>  70 loss ==>  0.048838679232597354\n",
            "epoch ==>  71 loss ==>  0.048765510516166684\n",
            "epoch ==>  72 loss ==>  0.0484411755824089\n",
            "epoch ==>  73 loss ==>  0.04827040419578552\n",
            "epoch ==>  74 loss ==>  0.048227545881271365\n",
            "epoch ==>  75 loss ==>  0.04820132259130478\n",
            "epoch ==>  76 loss ==>  0.048180925440788266\n",
            "epoch ==>  77 loss ==>  0.04816361726760864\n",
            "epoch ==>  78 loss ==>  0.048149791021347046\n",
            "epoch ==>  79 loss ==>  0.048137543182373045\n",
            "epoch ==>  80 loss ==>  0.04812471632003784\n",
            "epoch ==>  81 loss ==>  0.04811374037981033\n",
            "epoch ==>  82 loss ==>  0.04810286345720291\n",
            "epoch ==>  83 loss ==>  0.04809379960298538\n",
            "epoch ==>  84 loss ==>  0.04808267359733581\n",
            "epoch ==>  85 loss ==>  0.048073623616695406\n",
            "epoch ==>  86 loss ==>  0.04806299332380295\n",
            "epoch ==>  87 loss ==>  0.048055553262233736\n",
            "epoch ==>  88 loss ==>  0.04804743828058243\n",
            "epoch ==>  89 loss ==>  0.04803931705236435\n",
            "epoch ==>  90 loss ==>  0.04803230892419815\n",
            "epoch ==>  91 loss ==>  0.04802507741689682\n",
            "epoch ==>  92 loss ==>  0.04801666729211807\n",
            "epoch ==>  93 loss ==>  0.04800928600311279\n",
            "epoch ==>  94 loss ==>  0.048000653178691864\n",
            "epoch ==>  95 loss ==>  0.04799339626312256\n",
            "epoch ==>  96 loss ==>  0.04798681235074997\n",
            "epoch ==>  97 loss ==>  0.0479797381067276\n",
            "epoch ==>  98 loss ==>  0.0479757053399086\n",
            "epoch ==>  99 loss ==>  0.04796943615913391\n",
            "epoch ==>  100 loss ==>  0.04796303227424622\n",
            "epoch ==>  101 loss ==>  0.04795753105163574\n",
            "epoch ==>  102 loss ==>  0.04795061494112015\n",
            "epoch ==>  103 loss ==>  0.04794412305593491\n",
            "epoch ==>  104 loss ==>  0.04794067050457001\n",
            "epoch ==>  105 loss ==>  0.047933987538814546\n",
            "epoch ==>  106 loss ==>  0.04792911362171173\n",
            "epoch ==>  107 loss ==>  0.047922845420837405\n",
            "epoch ==>  108 loss ==>  0.047917190141677855\n",
            "epoch ==>  109 loss ==>  0.047911857426166535\n",
            "epoch ==>  110 loss ==>  0.04790713688373566\n",
            "epoch ==>  111 loss ==>  0.04790239698171615\n",
            "epoch ==>  112 loss ==>  0.04789587190389633\n",
            "epoch ==>  113 loss ==>  0.0478901259636879\n",
            "epoch ==>  114 loss ==>  0.047886685042381284\n",
            "epoch ==>  115 loss ==>  0.047880720932483675\n",
            "epoch ==>  116 loss ==>  0.04787474710941315\n",
            "epoch ==>  117 loss ==>  0.04787180672645569\n",
            "epoch ==>  118 loss ==>  0.04786496198654175\n",
            "epoch ==>  119 loss ==>  0.04786086643457413\n",
            "epoch ==>  120 loss ==>  0.047856369452476503\n",
            "epoch ==>  121 loss ==>  0.047851938660144804\n",
            "epoch ==>  122 loss ==>  0.04784796242713928\n",
            "epoch ==>  123 loss ==>  0.04784421636581421\n",
            "epoch ==>  124 loss ==>  0.04783998329877853\n",
            "epoch ==>  125 loss ==>  0.047836740276813505\n",
            "epoch ==>  126 loss ==>  0.04783259181022644\n",
            "epoch ==>  127 loss ==>  0.04782567405223846\n",
            "epoch ==>  128 loss ==>  0.04782281644582748\n",
            "epoch ==>  129 loss ==>  0.0478175163269043\n",
            "epoch ==>  130 loss ==>  0.04781391229867935\n",
            "epoch ==>  131 loss ==>  0.04781083209276199\n",
            "epoch ==>  132 loss ==>  0.047806224296092985\n",
            "epoch ==>  133 loss ==>  0.04780236839056015\n",
            "epoch ==>  134 loss ==>  0.047801249549388886\n",
            "epoch ==>  135 loss ==>  0.04779657091379166\n",
            "epoch ==>  136 loss ==>  0.04779265452623367\n",
            "epoch ==>  137 loss ==>  0.04778836857795715\n",
            "epoch ==>  138 loss ==>  0.04778371190786362\n",
            "epoch ==>  139 loss ==>  0.04778270049571991\n",
            "epoch ==>  140 loss ==>  0.04777829828739166\n",
            "epoch ==>  141 loss ==>  0.04777317040681839\n",
            "epoch ==>  142 loss ==>  0.047771964521408083\n",
            "epoch ==>  143 loss ==>  0.04776920616388321\n",
            "epoch ==>  144 loss ==>  0.047766142003536224\n",
            "epoch ==>  145 loss ==>  0.04776385339736938\n",
            "epoch ==>  146 loss ==>  0.04775602029561996\n",
            "epoch ==>  147 loss ==>  0.04775564685583115\n",
            "epoch ==>  148 loss ==>  0.047751253452301023\n",
            "epoch ==>  149 loss ==>  0.047747887332439426\n",
            "epoch ==>  150 loss ==>  0.04774611742496491\n",
            "epoch ==>  151 loss ==>  0.0477424461221695\n",
            "epoch ==>  152 loss ==>  0.0477419491648674\n",
            "epoch ==>  153 loss ==>  0.04774121065378189\n",
            "epoch ==>  154 loss ==>  0.04773706505060196\n",
            "epoch ==>  155 loss ==>  0.04773085240125656\n",
            "epoch ==>  156 loss ==>  0.04773216100931168\n",
            "epoch ==>  157 loss ==>  0.04772913272857666\n",
            "epoch ==>  158 loss ==>  0.04772417073726654\n",
            "epoch ==>  159 loss ==>  0.04772222307443619\n",
            "epoch ==>  160 loss ==>  0.047719649641513824\n",
            "epoch ==>  161 loss ==>  0.04771971744537354\n",
            "epoch ==>  162 loss ==>  0.047714766335487366\n",
            "epoch ==>  163 loss ==>  0.04771144655942917\n",
            "epoch ==>  164 loss ==>  0.04770773329496383\n",
            "epoch ==>  165 loss ==>  0.047705055689811704\n",
            "epoch ==>  166 loss ==>  0.0477016646027565\n",
            "epoch ==>  167 loss ==>  0.047702593626976014\n",
            "epoch ==>  168 loss ==>  0.04769463572502136\n",
            "epoch ==>  169 loss ==>  0.0476922466802597\n",
            "epoch ==>  170 loss ==>  0.04769091885328293\n",
            "epoch ==>  171 loss ==>  0.047688487770557404\n",
            "epoch ==>  172 loss ==>  0.04768654674291611\n",
            "epoch ==>  173 loss ==>  0.04768546729803085\n",
            "epoch ==>  174 loss ==>  0.04768249432086945\n",
            "epoch ==>  175 loss ==>  0.047678955521583556\n",
            "epoch ==>  176 loss ==>  0.04767453802347183\n",
            "epoch ==>  177 loss ==>  0.04767282051086426\n",
            "epoch ==>  178 loss ==>  0.047670029816627504\n",
            "epoch ==>  179 loss ==>  0.0476686852645874\n",
            "epoch ==>  180 loss ==>  0.04766586344242096\n",
            "epoch ==>  181 loss ==>  0.047661869969367984\n",
            "epoch ==>  182 loss ==>  0.0476592103433609\n",
            "epoch ==>  183 loss ==>  0.047659550123214725\n",
            "epoch ==>  184 loss ==>  0.04765394997119904\n",
            "epoch ==>  185 loss ==>  0.047655575387477875\n",
            "epoch ==>  186 loss ==>  0.047656793701648714\n",
            "epoch ==>  187 loss ==>  0.04765048554897308\n",
            "epoch ==>  188 loss ==>  0.04764619709968567\n",
            "epoch ==>  189 loss ==>  0.04764298616886139\n",
            "epoch ==>  190 loss ==>  0.047646348447799684\n",
            "epoch ==>  191 loss ==>  0.04764253535270691\n",
            "epoch ==>  192 loss ==>  0.04764100125551224\n",
            "epoch ==>  193 loss ==>  0.04763739804029465\n",
            "epoch ==>  194 loss ==>  0.04763941157102585\n",
            "epoch ==>  195 loss ==>  0.047637480523586274\n",
            "epoch ==>  196 loss ==>  0.04763067813873291\n",
            "epoch ==>  197 loss ==>  0.047629711792469026\n",
            "epoch ==>  198 loss ==>  0.04762822644710541\n",
            "epoch ==>  199 loss ==>  0.04762078618764877\n",
            "epoch ==>  200 loss ==>  0.047618628916740416\n",
            "epoch ==>  201 loss ==>  0.0476167279958725\n",
            "epoch ==>  202 loss ==>  0.0476230692076683\n",
            "epoch ==>  203 loss ==>  0.047611397614479065\n",
            "epoch ==>  204 loss ==>  0.047613316087722776\n",
            "epoch ==>  205 loss ==>  0.047606725833415985\n",
            "epoch ==>  206 loss ==>  0.04760711528539657\n",
            "epoch ==>  207 loss ==>  0.047603932266235355\n",
            "epoch ==>  208 loss ==>  0.047614149239063264\n",
            "epoch ==>  209 loss ==>  0.04760334398984909\n",
            "epoch ==>  210 loss ==>  0.04759952026128769\n",
            "epoch ==>  211 loss ==>  0.04759496020078659\n",
            "epoch ==>  212 loss ==>  0.047601487782001496\n",
            "epoch ==>  213 loss ==>  0.04759555376052856\n",
            "epoch ==>  214 loss ==>  0.04759901726484299\n",
            "epoch ==>  215 loss ==>  0.047591822729110715\n",
            "epoch ==>  216 loss ==>  0.047597144916057585\n",
            "epoch ==>  217 loss ==>  0.04758681398391724\n",
            "epoch ==>  218 loss ==>  0.04758607861995697\n",
            "epoch ==>  219 loss ==>  0.0475863975572586\n",
            "epoch ==>  220 loss ==>  0.04758086814880371\n",
            "epoch ==>  221 loss ==>  0.047576869254112246\n",
            "epoch ==>  222 loss ==>  0.04757855010747909\n",
            "epoch ==>  223 loss ==>  0.04757493993759155\n",
            "epoch ==>  224 loss ==>  0.04757477301359177\n",
            "epoch ==>  225 loss ==>  0.04757675498962402\n",
            "epoch ==>  226 loss ==>  0.04756867461442947\n",
            "epoch ==>  227 loss ==>  0.047570080852508544\n",
            "epoch ==>  228 loss ==>  0.04756520609378815\n",
            "epoch ==>  229 loss ==>  0.04756710075139999\n",
            "epoch ==>  230 loss ==>  0.04756553147077561\n",
            "epoch ==>  231 loss ==>  0.04756698984861374\n",
            "epoch ==>  232 loss ==>  0.04756642001390457\n",
            "epoch ==>  233 loss ==>  0.04755951805830002\n",
            "epoch ==>  234 loss ==>  0.047559869906902315\n",
            "epoch ==>  235 loss ==>  0.04756353076696396\n",
            "epoch ==>  236 loss ==>  0.047555428624153136\n",
            "epoch ==>  237 loss ==>  0.04755448906898498\n",
            "epoch ==>  238 loss ==>  0.04755275929450989\n",
            "epoch ==>  239 loss ==>  0.047548774766922\n",
            "epoch ==>  240 loss ==>  0.04755284430503845\n",
            "epoch ==>  241 loss ==>  0.04754779916524887\n",
            "epoch ==>  242 loss ==>  0.04754490332365036\n",
            "epoch ==>  243 loss ==>  0.047537248871326446\n",
            "epoch ==>  244 loss ==>  0.0475473133277893\n",
            "epoch ==>  245 loss ==>  0.04753777419567108\n",
            "epoch ==>  246 loss ==>  0.04753809708356857\n",
            "epoch ==>  247 loss ==>  0.04753468657493591\n",
            "epoch ==>  248 loss ==>  0.047531435735225676\n",
            "epoch ==>  249 loss ==>  0.047532138686180114\n",
            "epoch ==>  250 loss ==>  0.047530473358631135\n",
            "epoch ==>  251 loss ==>  0.04752889885425567\n",
            "epoch ==>  252 loss ==>  0.04752723362207413\n",
            "epoch ==>  253 loss ==>  0.04752704501390457\n",
            "epoch ==>  254 loss ==>  0.04752418998479843\n",
            "epoch ==>  255 loss ==>  0.04752543727397919\n",
            "epoch ==>  256 loss ==>  0.04752013485193252\n",
            "epoch ==>  257 loss ==>  0.047522279081344605\n",
            "epoch ==>  258 loss ==>  0.04751401150941849\n",
            "epoch ==>  259 loss ==>  0.047515091199874876\n",
            "epoch ==>  260 loss ==>  0.04751520248889923\n",
            "epoch ==>  261 loss ==>  0.04751375114679336\n",
            "epoch ==>  262 loss ==>  0.047510335664749145\n",
            "epoch ==>  263 loss ==>  0.04750597315311432\n",
            "epoch ==>  264 loss ==>  0.0475103965640068\n",
            "epoch ==>  265 loss ==>  0.04750684380531311\n",
            "epoch ==>  266 loss ==>  0.047513534734249116\n",
            "epoch ==>  267 loss ==>  0.04750588898181915\n",
            "epoch ==>  268 loss ==>  0.04750453239440918\n",
            "epoch ==>  269 loss ==>  0.04750477787733078\n",
            "epoch ==>  270 loss ==>  0.04750086725234985\n",
            "epoch ==>  271 loss ==>  0.04750462554216385\n",
            "epoch ==>  272 loss ==>  0.0474961480140686\n",
            "epoch ==>  273 loss ==>  0.04750157766819\n",
            "epoch ==>  274 loss ==>  0.047495668988227845\n",
            "epoch ==>  275 loss ==>  0.047493437287807465\n",
            "epoch ==>  276 loss ==>  0.0474923856139183\n",
            "epoch ==>  277 loss ==>  0.047495123550891874\n",
            "epoch ==>  278 loss ==>  0.047486405324935914\n",
            "epoch ==>  279 loss ==>  0.04748632429838181\n",
            "epoch ==>  280 loss ==>  0.047487244675159454\n",
            "epoch ==>  281 loss ==>  0.04748875678062439\n",
            "epoch ==>  282 loss ==>  0.047485095789432524\n",
            "epoch ==>  283 loss ==>  0.04748352385997772\n",
            "epoch ==>  284 loss ==>  0.04747608835935593\n",
            "epoch ==>  285 loss ==>  0.047492804021835325\n",
            "epoch ==>  286 loss ==>  0.0474865922999382\n",
            "epoch ==>  287 loss ==>  0.047479272813797\n",
            "epoch ==>  288 loss ==>  0.04748392779588699\n",
            "epoch ==>  289 loss ==>  0.047484194180965424\n",
            "epoch ==>  290 loss ==>  0.04747917302131653\n",
            "epoch ==>  291 loss ==>  0.047477537939548495\n",
            "epoch ==>  292 loss ==>  0.04747915069103241\n",
            "epoch ==>  293 loss ==>  0.04747536707639694\n",
            "epoch ==>  294 loss ==>  0.0474854417181015\n",
            "epoch ==>  295 loss ==>  0.047481038756370546\n",
            "epoch ==>  296 loss ==>  0.04747887711286545\n",
            "epoch ==>  297 loss ==>  0.04747334086894989\n",
            "epoch ==>  298 loss ==>  0.04747107762813568\n",
            "epoch ==>  299 loss ==>  0.04747633746623993\n",
            "epoch ==>  300 loss ==>  0.047477636296749115\n",
            "epoch ==>  301 loss ==>  0.04747585398435593\n",
            "epoch ==>  302 loss ==>  0.04746807551860809\n",
            "epoch ==>  303 loss ==>  0.04746243287801743\n",
            "epoch ==>  304 loss ==>  0.04746589509487152\n",
            "epoch ==>  305 loss ==>  0.04746435553789139\n",
            "epoch ==>  306 loss ==>  0.04745942286729812\n",
            "epoch ==>  307 loss ==>  0.047464689133167266\n",
            "epoch ==>  308 loss ==>  0.04745618143796921\n",
            "epoch ==>  309 loss ==>  0.04746238417387009\n",
            "epoch ==>  310 loss ==>  0.047465434782505034\n",
            "epoch ==>  311 loss ==>  0.04746512018442154\n",
            "epoch ==>  312 loss ==>  0.04745859137296676\n",
            "epoch ==>  313 loss ==>  0.04745452516317367\n",
            "epoch ==>  314 loss ==>  0.04745936154127121\n",
            "epoch ==>  315 loss ==>  0.0474561337685585\n",
            "epoch ==>  316 loss ==>  0.04745681223392487\n",
            "epoch ==>  317 loss ==>  0.04745491743564606\n",
            "epoch ==>  318 loss ==>  0.047452591700553895\n",
            "epoch ==>  319 loss ==>  0.047453278877735136\n",
            "epoch ==>  320 loss ==>  0.04744905879497528\n",
            "epoch ==>  321 loss ==>  0.04744639521360397\n",
            "epoch ==>  322 loss ==>  0.0474403820681572\n",
            "epoch ==>  323 loss ==>  0.047445234553813935\n",
            "epoch ==>  324 loss ==>  0.047443329586982726\n",
            "epoch ==>  325 loss ==>  0.047443823428153994\n",
            "epoch ==>  326 loss ==>  0.04743907107114792\n",
            "epoch ==>  327 loss ==>  0.047435805587768554\n",
            "epoch ==>  328 loss ==>  0.04744069349527359\n",
            "epoch ==>  329 loss ==>  0.0474413109588623\n",
            "epoch ==>  330 loss ==>  0.04743830601930618\n",
            "epoch ==>  331 loss ==>  0.047442091021537784\n",
            "epoch ==>  332 loss ==>  0.04743244571208954\n",
            "epoch ==>  333 loss ==>  0.04743459414005279\n",
            "epoch ==>  334 loss ==>  0.04743041828870773\n",
            "epoch ==>  335 loss ==>  0.04743282244682312\n",
            "epoch ==>  336 loss ==>  0.04742668082237244\n",
            "epoch ==>  337 loss ==>  0.04742619710445404\n",
            "epoch ==>  338 loss ==>  0.0474337482047081\n",
            "epoch ==>  339 loss ==>  0.04742745242834091\n",
            "epoch ==>  340 loss ==>  0.047423701679706574\n",
            "epoch ==>  341 loss ==>  0.04742077169656753\n",
            "epoch ==>  342 loss ==>  0.04741861324310303\n",
            "epoch ==>  343 loss ==>  0.047417261357307436\n",
            "epoch ==>  344 loss ==>  0.04742000365257263\n",
            "epoch ==>  345 loss ==>  0.0474207150220871\n",
            "epoch ==>  346 loss ==>  0.0474218821978569\n",
            "epoch ==>  347 loss ==>  0.04741600942850113\n",
            "epoch ==>  348 loss ==>  0.04741801481246948\n",
            "epoch ==>  349 loss ==>  0.04741884156703949\n",
            "epoch ==>  350 loss ==>  0.047415624022483824\n",
            "epoch ==>  351 loss ==>  0.04741702932834625\n",
            "epoch ==>  352 loss ==>  0.04742025396347046\n",
            "epoch ==>  353 loss ==>  0.04741783974885941\n",
            "epoch ==>  354 loss ==>  0.04740908987283707\n",
            "epoch ==>  355 loss ==>  0.04742162072896958\n",
            "epoch ==>  356 loss ==>  0.04741755835294723\n",
            "epoch ==>  357 loss ==>  0.047420147793293\n",
            "epoch ==>  358 loss ==>  0.04741257508516312\n",
            "epoch ==>  359 loss ==>  0.04741020037651062\n",
            "epoch ==>  360 loss ==>  0.04741008147001267\n",
            "epoch ==>  361 loss ==>  0.047407920892238616\n",
            "epoch ==>  362 loss ==>  0.04740512075662613\n",
            "epoch ==>  363 loss ==>  0.04740658071994781\n",
            "epoch ==>  364 loss ==>  0.047399387934207914\n",
            "epoch ==>  365 loss ==>  0.047400785593986514\n",
            "epoch ==>  366 loss ==>  0.04739712293148041\n",
            "epoch ==>  367 loss ==>  0.047402481572628025\n",
            "epoch ==>  368 loss ==>  0.04740379224777222\n",
            "epoch ==>  369 loss ==>  0.047397623710632324\n",
            "epoch ==>  370 loss ==>  0.04740189880132675\n",
            "epoch ==>  371 loss ==>  0.047406421375274656\n",
            "epoch ==>  372 loss ==>  0.04740034569501877\n",
            "epoch ==>  373 loss ==>  0.047400217368602754\n",
            "epoch ==>  374 loss ==>  0.04739478249549866\n",
            "epoch ==>  375 loss ==>  0.047402468860149385\n",
            "epoch ==>  376 loss ==>  0.047399501571655275\n",
            "epoch ==>  377 loss ==>  0.047391637125015255\n",
            "epoch ==>  378 loss ==>  0.04739472806930542\n",
            "epoch ==>  379 loss ==>  0.0473915593290329\n",
            "epoch ==>  380 loss ==>  0.04739340698003769\n",
            "epoch ==>  381 loss ==>  0.04738915977716446\n",
            "epoch ==>  382 loss ==>  0.04738691784381866\n",
            "epoch ==>  383 loss ==>  0.047399111177921296\n",
            "epoch ==>  384 loss ==>  0.04739211777210236\n",
            "epoch ==>  385 loss ==>  0.04739411159992218\n",
            "epoch ==>  386 loss ==>  0.047381496841907504\n",
            "epoch ==>  387 loss ==>  0.0473855775308609\n",
            "epoch ==>  388 loss ==>  0.04738120275735855\n",
            "epoch ==>  389 loss ==>  0.047377693996429444\n",
            "epoch ==>  390 loss ==>  0.047379795849323274\n",
            "epoch ==>  391 loss ==>  0.04738840052366257\n",
            "epoch ==>  392 loss ==>  0.04737673686265945\n",
            "epoch ==>  393 loss ==>  0.047387701263427735\n",
            "epoch ==>  394 loss ==>  0.04737473175287247\n",
            "epoch ==>  395 loss ==>  0.04738645994663238\n",
            "epoch ==>  396 loss ==>  0.04737777584075928\n",
            "epoch ==>  397 loss ==>  0.047385167424678805\n",
            "epoch ==>  398 loss ==>  0.047376518065929414\n",
            "epoch ==>  399 loss ==>  0.047380143423080445\n",
            "epoch ==>  400 loss ==>  0.047382203590869906\n",
            "epoch ==>  401 loss ==>  0.04738962272882462\n",
            "epoch ==>  402 loss ==>  0.047382364177703855\n",
            "epoch ==>  403 loss ==>  0.04737825962781906\n",
            "epoch ==>  404 loss ==>  0.047380905787944794\n",
            "epoch ==>  405 loss ==>  0.04737441023111343\n",
            "epoch ==>  406 loss ==>  0.047368091838359834\n",
            "epoch ==>  407 loss ==>  0.04737881323337555\n",
            "epoch ==>  408 loss ==>  0.047374663169384004\n",
            "epoch ==>  409 loss ==>  0.04737579857349396\n",
            "epoch ==>  410 loss ==>  0.04737043459177017\n",
            "epoch ==>  411 loss ==>  0.04736298318386078\n",
            "epoch ==>  412 loss ==>  0.047372851910591124\n",
            "epoch ==>  413 loss ==>  0.047362944247722624\n",
            "epoch ==>  414 loss ==>  0.04736087715864182\n",
            "epoch ==>  415 loss ==>  0.04736170389890671\n",
            "epoch ==>  416 loss ==>  0.04736315775871277\n",
            "epoch ==>  417 loss ==>  0.04735763875007629\n",
            "epoch ==>  418 loss ==>  0.047369410111904146\n",
            "epoch ==>  419 loss ==>  0.04736519635438919\n",
            "epoch ==>  420 loss ==>  0.04736291269779205\n",
            "epoch ==>  421 loss ==>  0.04735419777154923\n",
            "epoch ==>  422 loss ==>  0.04735715342521667\n",
            "epoch ==>  423 loss ==>  0.04735444503307343\n",
            "epoch ==>  424 loss ==>  0.047362466843128205\n",
            "epoch ==>  425 loss ==>  0.04735195923089981\n",
            "epoch ==>  426 loss ==>  0.04735825499296188\n",
            "epoch ==>  427 loss ==>  0.047347729530334476\n",
            "epoch ==>  428 loss ==>  0.047352111372947696\n",
            "epoch ==>  429 loss ==>  0.047352620205879214\n",
            "epoch ==>  430 loss ==>  0.04735278143882751\n",
            "epoch ==>  431 loss ==>  0.04734484469175339\n",
            "epoch ==>  432 loss ==>  0.0473446622800827\n",
            "epoch ==>  433 loss ==>  0.04735316528081894\n",
            "epoch ==>  434 loss ==>  0.04734541410207748\n",
            "epoch ==>  435 loss ==>  0.04734686416149139\n",
            "epoch ==>  436 loss ==>  0.04734718751192093\n",
            "epoch ==>  437 loss ==>  0.04734046439170837\n",
            "epoch ==>  438 loss ==>  0.04734257021903992\n",
            "epoch ==>  439 loss ==>  0.04735082623958588\n",
            "epoch ==>  440 loss ==>  0.04734752481460571\n",
            "epoch ==>  441 loss ==>  0.0473488791847229\n",
            "epoch ==>  442 loss ==>  0.04734286298036575\n",
            "epoch ==>  443 loss ==>  0.047336735167503356\n",
            "epoch ==>  444 loss ==>  0.04733940691947937\n",
            "epoch ==>  445 loss ==>  0.04733656511068344\n",
            "epoch ==>  446 loss ==>  0.04733894474506378\n",
            "epoch ==>  447 loss ==>  0.04734142381668091\n",
            "epoch ==>  448 loss ==>  0.04733277443408966\n",
            "epoch ==>  449 loss ==>  0.04734120285511017\n",
            "epoch ==>  450 loss ==>  0.047331316177845\n",
            "epoch ==>  451 loss ==>  0.04733459966659546\n",
            "epoch ==>  452 loss ==>  0.0473325663113594\n",
            "epoch ==>  453 loss ==>  0.04735828531742096\n",
            "epoch ==>  454 loss ==>  0.047337941193580625\n",
            "epoch ==>  455 loss ==>  0.047340784018039705\n",
            "epoch ==>  456 loss ==>  0.04733051147222519\n",
            "epoch ==>  457 loss ==>  0.047334632563591\n",
            "epoch ==>  458 loss ==>  0.047330196352005006\n",
            "epoch ==>  459 loss ==>  0.047326348202228544\n",
            "epoch ==>  460 loss ==>  0.04733584230184555\n",
            "epoch ==>  461 loss ==>  0.04732905656337738\n",
            "epoch ==>  462 loss ==>  0.04733036587953567\n",
            "epoch ==>  463 loss ==>  0.04732101284980774\n",
            "epoch ==>  464 loss ==>  0.04732788701057434\n",
            "epoch ==>  465 loss ==>  0.047325331304073334\n",
            "epoch ==>  466 loss ==>  0.04732320039749145\n",
            "epoch ==>  467 loss ==>  0.04732940551042557\n",
            "epoch ==>  468 loss ==>  0.0473341672372818\n",
            "epoch ==>  469 loss ==>  0.04731798961162567\n",
            "epoch ==>  470 loss ==>  0.04731934071302414\n",
            "epoch ==>  471 loss ==>  0.04732053213119507\n",
            "epoch ==>  472 loss ==>  0.04731734803676605\n",
            "epoch ==>  473 loss ==>  0.047317395720481874\n",
            "epoch ==>  474 loss ==>  0.0473231771326065\n",
            "epoch ==>  475 loss ==>  0.04731533945798874\n",
            "epoch ==>  476 loss ==>  0.047321061022281646\n",
            "epoch ==>  477 loss ==>  0.047323694326877594\n",
            "epoch ==>  478 loss ==>  0.04730998454332352\n",
            "epoch ==>  479 loss ==>  0.047313007349967955\n",
            "epoch ==>  480 loss ==>  0.04731097059726715\n",
            "epoch ==>  481 loss ==>  0.0473086066198349\n",
            "epoch ==>  482 loss ==>  0.04731334856510162\n",
            "epoch ==>  483 loss ==>  0.04730928915739059\n",
            "epoch ==>  484 loss ==>  0.047309548785686495\n",
            "epoch ==>  485 loss ==>  0.0473067276096344\n",
            "epoch ==>  486 loss ==>  0.047300281007289884\n",
            "epoch ==>  487 loss ==>  0.04731918546438217\n",
            "epoch ==>  488 loss ==>  0.047310887382030486\n",
            "epoch ==>  489 loss ==>  0.04731974019050598\n",
            "epoch ==>  490 loss ==>  0.04731433723449707\n",
            "epoch ==>  491 loss ==>  0.0473057616353035\n",
            "epoch ==>  492 loss ==>  0.04730772251844406\n",
            "epoch ==>  493 loss ==>  0.04730836766242981\n",
            "epoch ==>  494 loss ==>  0.047308007118701936\n",
            "epoch ==>  495 loss ==>  0.04730959966421127\n",
            "epoch ==>  496 loss ==>  0.04730478816747665\n",
            "epoch ==>  497 loss ==>  0.04730470300197601\n",
            "epoch ==>  498 loss ==>  0.04730347063064575\n",
            "epoch ==>  499 loss ==>  0.047298584876060486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjV6Vvdwh7YL"
      },
      "source": [
        "## ToDo: use matplot lib to plot the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8RV3mWbicLV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6d93152b-c0d1-4294-cb9d-001b3724c23a"
      },
      "source": [
        "# Your code should ne here\n",
        "plt.plot(losses)\n",
        "plt.title(\"loss against epochs\")\n",
        "plt.xlabel(\"No epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaTV06M9ifwV"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBTXQJgxinj_"
      },
      "source": [
        "Now that we have trained our model, we need to know how much this model will perform (in term of accuracy) on the validation set.\n",
        "\n",
        "The performance on the validation set should give us a good hint, on how well our model will perform on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtcLb9uASNo4"
      },
      "source": [
        "# Run this cell\n",
        "\n",
        "BS = 256 \n",
        "N_val = x_val.shape[0]\n",
        "\n",
        "if N_val % BS == 0:\n",
        "  n_batches = int(N_val / BS)\n",
        "else:\n",
        "  n_batches = int(N_val / BS) + 1"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvreKsQVju9z"
      },
      "source": [
        "#### ToDo: fill in the missing lines to compute the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8EIkvmIwfxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5a6e6f-7761-425c-ff57-52c35c7a0316"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# Add the no grad context manager (covered in the tutorial)\n",
        "with torch.no_grad():  \n",
        "  model.eval()   \n",
        "  for batch in range(n_batches):\n",
        "    \n",
        "    start = batch * BS\n",
        "    end = start + BS\n",
        "\n",
        "    val_images = x_val[start:end]\n",
        "    val_labels = y_val[start:end]\n",
        "\n",
        "    val_images =val_images.to(device)        # change the device of val_images to gpu\n",
        "    val_labels = val_labels.to(device)       # change the device of val_labels to gpu\n",
        "    \n",
        "\n",
        "    outputs =model(val_images)           # feed the val_images to the model to get the outputs\n",
        "    \n",
        "\n",
        "    predicted =torch.max(outputs, 1)[1].to(device)         # compute the argmax of the outputs to get the predicted labels\n",
        "    \n",
        "    total += val_labels.size(0)\n",
        "    \n",
        "    correct += (predicted == val_labels).sum().item()\n",
        "    batch+=1\n",
        "\n",
        "print('Accuracy of the network on the %d test images: %d %%' % (\n",
        "    total, 100 * correct / total))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 86 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gx1i2p51IYC",
        "outputId": "4560e9a7-90b4-4ce7-e85e-e1cda51182ea"
      },
      "source": [
        "predicted[0:100]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 2, 1, 4, 7, 4, 6, 8, 5, 9, 3, 9, 1, 4, 6, 5, 7, 1, 1, 9, 2, 8, 5,\n",
              "        9, 9, 9, 2, 3, 3, 3, 0, 6, 2, 9, 7, 5, 7, 9, 4, 1, 3, 9, 7, 6, 9, 9, 5,\n",
              "        4, 4, 0, 0, 3, 9, 1, 1, 4, 7, 8, 3, 0, 3, 4, 8, 2, 3, 3, 3, 8, 0, 8, 2,\n",
              "        5, 6, 2, 1, 9, 5, 2, 4, 1, 0, 5, 4, 0, 3, 3, 7, 3, 1, 0, 2, 7, 5, 7, 7,\n",
              "        2, 2, 3, 9], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xBOUjFG1L0a",
        "outputId": "60bb37e2-d626-4a75-cd7b-f7d0c3c569df"
      },
      "source": [
        "val_labels[0:100]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 4, 1, 4, 7, 4, 0, 8, 8, 5, 3, 9, 1, 4, 4, 5, 7, 1, 1, 7, 2, 8, 5,\n",
              "        9, 9, 9, 2, 3, 3, 3, 0, 6, 4, 9, 7, 5, 7, 9, 4, 1, 3, 9, 7, 6, 9, 9, 5,\n",
              "        4, 4, 0, 6, 3, 9, 1, 1, 4, 7, 8, 3, 0, 3, 4, 8, 2, 3, 3, 3, 8, 6, 8, 6,\n",
              "        5, 6, 4, 1, 9, 5, 2, 4, 1, 0, 5, 2, 0, 6, 6, 7, 3, 1, 0, 3, 7, 5, 7, 7,\n",
              "        3, 2, 6, 9], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e7ZbzuN-DK-",
        "outputId": "e8267de7-a77e-411c-d9b9-62a0246cf1f4"
      },
      "source": [
        "correct/total"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O89dx5apqnal"
      },
      "source": [
        "# You are expected to get good val acc > 70% at least"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPzjRtH6qtrg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}